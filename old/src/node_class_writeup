I'm trying to create nodes, with each node representing a neural network layer, or a group of neural network layers.   

Each node has a unique ID upon creation. Each node class contains: in_dim and out_dim, name, one input_node and one output_node. 

All layer classes inherit from the Node class should implement infer_out_dim_from_in_dim and infer_in_dim_from_out_dim, to_pytorch_code and __str__ 
The __str__ method for specific layer class should print the layer type and 


Frontend 
When a user attempts to add a node at the “front end”, the “front end” will send the information of the node to the backend: layer type and layer parameters if it’s a layer(if it’s a node with aggregated layers) it would not have this information, and relative position (front, back) to an existing node, or two existing nodes. 
If the user decides to create an independent node, then it won’t have a relative position to an existing node, but the user will be prompted to enter one or both the in_dim and the out_dim definition for this layer. 

Backend 
The node can be a single layer but it can also be multiple layers aggregated by the user, in that case it will retain both in_dim and out_dim information but no layer type and layer params, and we won’t be able to help the user infer out_dim from in_dim and infer in_dim from out_dim. 

The first thing we need to check, when the user attempts to add a node to the front and back of an existing node, or in between two existing nodes, is that whether the out_node of an in_node or the in_node of an out_node had already been defined. We need to maintain that each node has only one in_node and one out_node for the nodes to form a valid sequence. 

Although we can enforce this on the front end, by not displaying the connect-to button to one side of the node if it already has something connected on that side, we still need to perform this check in the backend. 

Then we need to check for potential dimension match issues. 

There are multiple ways node (or layer) dimension match can happen. 
Think about what the user wants to do when building a neural network. 
They can build it from input to output. or have a few blocks ready and connect them, or they may want to change a layer in between. 


1. Input to output 
In the first case, the dimension match happens when the user tries to add a node.
In this case the input_node is defined, the output_node is not. 

When the user adds the node, they can already have the input_dim, 
then we are going to check the input_dim and see if the input_node's output dimension matches with that.
If not we throw an error, and ask them if they want to use the output_dim of the input_node instead. (Maybe we don’t do this yet, or it can happen on the front end)  

If they don't define an input_dim, we will assume that they want the input_dim to be the same as the output_dim of the input_node. Then we can perform an infer_out_dim_from_in_dim specific to the layer type, and if it cannot produce a valid out_dim, we will report an error and ask the user to redefined the layer parameters. 

2. Connect two existing blocks 
When the user wants to connect two existing blocks. The input node and out node are both defined. 

If the input_dim and output_dim of the added new layer are already defined by the user, then we just do a check on both input_dim match with the input_node output_dim, and output_dim match with the output_node input_dim and throw errors respectively. (This can happen on front-end) 

 If one of them is undefined or both input_dim and output_dim are not defined, we will infer them from both sides then perform an infer_out_dim_from_in_dim to see if the input_dim is valid and if the resulting output_dim actually equals to the defined output_dim. (The infer part will happen at the backend but the check part will happen in the front-end) If the input_dim is not valid, we have to ask the user to redefine the layer parameters. If the output_dim do not equal to each other, we have to ask the user to redefine the layer parameters as well. (The infer part will happen at the back end and the match part will happen at the front end) 


(Since there are sometimes multiple layer parameters that match an input_dim and an output_dim, maybe here we can give parameters suggestions based on the layer -type user choose, sometimes a reshape layer can be suggested) (We can also potentially spawn up a graph interface visualizing this particular layer in 3D as a visual aid for the user) 

Changing a Defined Layer or adding an additional layer between two connected layers are the same as connecting two existing blocks. Even though at the front-end this may be displayed differently. 


4. Output to Input 
Sometimes people may be weird and want to build from Output to Input, in this case the output_node is defined and the input_node is not. 

When the user adds the node, they can define an output_dim, 
then we are going to check the output_dim and see if the output_node's input dimension matches with that.
If not we throw an error, and ask them if they want to use the input_dim of the output node instead.   (Maybe we don’t do this yet, or it can happen on the front end)  

If they don't define an output_dim, we will assume that they want the output_dim to be the same as the input_dim of the output_node. Then we can perform an infer_in_dim_from_out_dim specific to the layer type, and if it cannot produce a valid out_dim, we will report an error and ask the user to redefine the layer parameters 


Layers:

Reshape (Flattenings are considered reshape )
Linear
ElementWiseNonlinearity 
NonLinearity1D
Conv1D
Conv2D
Conv3D
PoolNode1D
- MaxPool1D
- AvgPool1D
- LPPool1D
PoolNode2D
- MaxPool2D
- AvgPool2D
- LPPool2D
PoolNode3D
- MaxPool3D
- AvgPool3D
- LPPool3D

AdaptivePool1D
   - AdaptiveMaxPool1D
   - AdaptiveAveragePool1D

AdaptivePool2D
   - AdaptiveMaxPool2D
   - AdaptiveAveragePool2D
   
AdaptivePool3D
   - AdaptiveMaxPool3D
   - AdaptiveAveragePool3D


We have explained how nodes-building work. Now let’s see how we will build a seq. This will typically happen when the user is happy with what they have and now wants to generate some pytorch code. 

By applying the seq operations over some nodes, they will make all nodes in the seq immutable, this will allow the user to one click generate the corresponding PyTorch code. 



they can ofc unpack the seq again into nodes and edit them.