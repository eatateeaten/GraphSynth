Sounds like a pragmatic move. Here‚Äôs a roadmap for keeping your visual compiler tightly-coupled to the PyTorch ecosystem while still leaving room for StableHLO / MLIR back-ends.

1‚ÄÇCapture: stay upstream-native

What to capture	PyTorch hook that gives you it	Why it‚Äôs perfect for a visual front end
Eager modules & scripted helpers	torch.fx.symbolic_trace (fast, minimal requirements)	Gives you nested call_module nodes with fully-qualified names‚Äîeasy to map back onto boxes in a GUI.
Dynamic shapes / data-dependent branches	torch._dynamo.export (2.1+) or torch.export (2.2+)	Emits a shape-safe FX graph with Guards + functional ops; you won‚Äôt lose branches taken on other inputs.
Gradients for training views	functorch.compile.aot_function (AOT Autograd)	Hand-built forward & backward FX graphs‚Äîno need to derive adjoints later.
Tensor-only kernels for inference	torch.compile(..., backend="inductor")	Inductor‚Äôs intermediate ATen graph can be inspected, then resurfaced as SSA if you want to show ‚Äúpost-fusion‚Äù view.
Tip: build a pluggable capture fa√ßade (pyt_capture(mode="fx"|"export"|"aot")) so advanced users can swap in a richer graph without touching the GUI code.
2‚ÄÇIntermediate representation: map FX ‚Üí Your IR

Node mapping
call_module ‚Üí OpNode tagged with qual_name (e.g. block.bn) + param handles
call_function / call_method ‚Üí PrimitiveNode (e.g. torch.relu)
get_attr ‚Üí ConstantParamNode (weights/buffers)
Edge mapping
FX edges are positional. Store them explicitly so the visual editor can re-lay out wires when users drag blocks around.
SSA re-write
Run a pass that renames outputs %tmp0 ‚Ä¶ so that every tensor gets a stable ID‚Äîthis keeps the TypeScript side deterministic even after a user shuffles nodes.
Metadata you‚Äôll be glad you kept
{
  "path": "block.bn",      // for breadcrumbs
  "dtype": "f32",
  "shape": [N, C, H, W],
  "requires_grad": true,
  "fx_node_idx": 42
}
3‚ÄÇTransform passes (visual + CLI)

Pass	Why users care	PyTorch helper
Functionalization / alias clean-up	Makes in-place ops explicit‚Äîeasier to reason about graph purity.	torch._functorch._functionalize
Constant folding & CSE	Users instantly see dead-weight nodes disappear.	torch.fx.passes.shape_prop, torch.fx.passes.fold_constants
Operator fusion preview	Show the groups Inductor will fuse (nice colour overlay).	torch._inductor.fx_utils.extract_graph_fusion_groups
StableHLO legalisation	Let users target IREE / TPU.	torch_xla.experimental.exported_program_to_stablehlo after your edits
Because your IR retains the fx_node_idx, you can bounce back and forth: do a transformation in Python, sync the patch into the TypeScript canvas, and vice-versa.

4‚ÄÇRound-tripping back to executable PyTorch

IR ‚Üí FX: serialise your edited graph as a list of node records; rebuild an fx.Graph in Python.
Re-attach the original sub-modules with gm.add_submodule(path, mod).
FX ‚Üí TorchScript / Inductor: just call torch.export on the reconstructed GraphModule.
Optional: hand the FX graph to torch-mlir if you want MLIR but not StableHLO.
This keeps your compiler source-compatible with future PyTorch releases; even if Inductor‚Äôs internals change, FX graphs stay stable.
5‚ÄÇWhy PyTorch is a sweet spot for a visual compiler

Massive op coverage: the functional ATen ops touched by FX already map 1-to-1 to high-level math blocks‚Äîno need for custom icons.
Stable procedural API: FX and Dynamo are public and under backward-compat guarantees.
Ecosystem gravity: diffusers, TorchVision, Lightning, etc., all output modules you can trace unchanged.
Inter-IR bridges: torch-xla ‚Üí StableHLO, torch-mlir ‚Üí MHLO, ONNX exporters‚Äîyour back-end menu grows ‚Äúfor free.‚Äù
6‚ÄÇShort sprint plan

Week	Deliverable
1	Minimal FX importer ‚Üí draw graph in canvas with nested groups.
2	SSA renamer + constant-shape annotator pass.
3	GUI action: ‚Äúinline selected Module‚Äù (expand a sub-graph) and ‚Äúfold constants‚Äù.
4	Export path: edited IR ‚Üí FX ‚Üí torch.export ‚Üí StableHLO file.
5	Proof-of-concept fuse-preview overlay using Inductor‚Äôs fusion sets.
You‚Äôll have a demo where a user drags a ResNet on-screen, collapses blocks, folds constants, and hits ‚ÄúExport to StableHLO‚Äù‚Äîall inside one month of focused hacking.

Bottom line
Anchoring the front end on PyTorch FX + Dynamo gives you:

Deep, recoverable nesting for an intuitive visual hierarchy.
Ready-made passes you can expose as one-click refactors.
Easy egress to StableHLO, ONNX, or TorchScript without re-implementing export logic.
When you start wiring the canvas to Python, let me know‚ÄîI can help craft the exact JSON schema for node-edge dumps or show how to splice Inductor‚Äôs fusion annotations into your TypeScript renderer. üöÄ