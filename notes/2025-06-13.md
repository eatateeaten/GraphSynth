## Design notes - June 13, 2025
MODULEDB
1. Most of the UI and backend modules are missing from the new ModuleDB format. Need to add them back. 

2. We should define a "moduleType" enum so that we can go from ModuleDef to backend classes.

3. Currently we are matching the operations manually from frameworks to our Ops Class Hierarchy Tree. 

We may be able to generate and update the ModuleDB schema automatically from PyTorch and JAX frameworks using LLMs (DsPy?)
Need a set of rules/heuristics to determine moduleType. We can also add integration tests
to see if our moduledefs have the same behavior as the corresponding PyTorch/Jax modules.

4. More fields in the "DB" may be needed for certain moduleTypes to describe shape matching / inference
behaviors better but we should try our best to avoid that.

TRACER AND MLIR
5. We are going to have nested modules and once they are collapsed / saved we need to keep a serialized version of the module
for inclusion in other modules (MultiLayered DAG compiler)
The lowest layer we want to roll out to is primitive matrix operations

6. Jobs beyond that are AI infra optimization's domain. Doesn't mean we can't do it here
though. Some thoughts: 
For anything below primitive matrix operation that involving tiling, loop unrolling and loop skewing. We ought to do a restricted, hierarchical code rollout (level by level). Otherwise the search space would be infinite. 
Ideally we would recursively do NoN expansions until we reach singular flops [top-down] then fuse 
upwards [bottom-up]. I'm not gonna touch that because AI infra and MLSys people are already on it. 

(For modules above primitive matrix ops it doesn't make sense to
do the restricted hierarchical code rollout because the user should have the liberty to define the module's implementation
at any level if they want to)  


6. Tensor class can be renamed to something more generic so that it better represents a general temporary data storage 


ARCHITECUTRES WITH FEEDBACK LOOPS
7. Although our graphs do not allow cycles,
To do LSTM and RNN and other architectures containing feedback loops: 
It can be achieved through the training routine graph. 
We can copy the model structure a couple times across a few time stamps 
User can define a recursion relation across multiple time stamps with this view, by drawing edges between "tensors"  
And then can have something like RNN or LSTM 

CHALLENGES OF THIS PROJECT
The chellenge of our project is to combine HCI, system Design while integrating libraries and ensuring the mathematical correctness of the graph compiler (or more specifically the DAG compiler) 

Essentially we are going to have one DAG Compiler that rules them all, an IR for everything from top level modules down to infra and we are going to have a "tracer and code-regeneration" broomstick to cleanup all the MLE codebases

With this much time we could have made 100 LLM wrapper apps and get a lot of money. Rip I guess 


